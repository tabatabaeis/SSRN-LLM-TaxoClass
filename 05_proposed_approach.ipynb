{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18abd232-1b87-4704-bab0-b003c6ed93b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook evaluates the top taxonomy labels for each document by querying ChatGPT to determine if each label is a suitable fit. The results are stored as a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb963c5a-33ea-4194-8c73-78bc039fec54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543a1147-8ad5-40e1-a06e-fda1c17251cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e99303-75d0-4539-8918-d774aac6d8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  \n",
    "import re  \n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "import random  \n",
    "import json  \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca204b9-195f-4d2d-982c-c223e2352ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conf = DataBricksDevConfig\n",
    "\n",
    "path_results = conf.Data.path_results\n",
    "path_taxonomy = conf.Data.path_taxonomy\n",
    "folder_name = conf.Data.folder_name\n",
    "dic_abbreviation2full = conf.taxonomy_info.dic_abbreviation2full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b30ace2-9fb5-4ee5-b56b-6941fcfd8143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_in = pd.read_pickle(path_results + '4-ShortenTaxonomy.pckl')\n",
    "except Exception as e:\n",
    "    print(\"Cluster could not read the file (1- wrong cluster 2- no new json file)\",e)\n",
    "    dbutils.notebook.exit(str(e))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90b3ac0-0bf3-42a5-ae85-afb4a63076d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Taxonomy from generated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8afa521-3fba-440f-8712-d73cf4f7c992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(path_taxonomy + 'dic_taxonomy_embeddings.pickle', 'rb') as file:\n",
    "    dic_taxonomy = pickle.load(file)\n",
    "dic_taxonomy_id2label = {dic_taxonomy[label]['id']:label for label in dic_taxonomy.keys()}\n",
    "labels_all = list(dic_taxonomy.keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead78123-2b66-49e2-8780-3a3197b46989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_label(label):\n",
    "    # return((label+''))    \n",
    "    cleaned_label = label\n",
    "    for abbreviation, expansion in dic_abbreviation2full.items():\n",
    "        # if abbreviation in cleaned_label:\n",
    "        if cleaned_label.startswith(abbreviation):\n",
    "            cleaned_label = cleaned_label.replace(abbreviation, expansion)\n",
    "    return cleaned_label  \n",
    "\n",
    "for k,v in dic_taxonomy.items():\n",
    "    dic_taxonomy[k]['cleaned_name']= clean_label(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25dc2a1-c967-40f6-8594-4db363bb2ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# required functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157ecf8e-0ad9-4c5a-9284-e46b842c7501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1- Ask about the label itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3625d71f-aecf-48d0-8ce7-cc902f3a9a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_system_template_label = \"You are an AI trained to evaluate the relevance of a label for a given SSRN pre-print document. You will receive the document's title, keywords, abstract, and the label's ID, name and description. Your task is to determine if the label is a good fit for the document. A label fits well if the document's main focus aligns with the area the label describes. Your output should be a concise JSON object. The JSON object should contain three keys: \\\"main_focus\\\", a very short representation of the document's main focus, \\\"label_fit\\\", representing the fit as a boolean value. It's crucial to utilize the entire scoring range to reflect varying degrees of relevancy. Please do not provide no further information or explanation, in addition to the json object. Do not use the slash or backslash characters in your output.\"\n",
    "\n",
    "def F_prompt_label (row) :\n",
    "    label = row['label']\n",
    "    label_ID = dic_taxonomy[label]['id']\n",
    "    cleaned_name = dic_taxonomy[label]['cleaned_name']\n",
    "    if len(dic_taxonomy[label]['description_cleaned'])>0:\n",
    "        description = dic_taxonomy[label]['description_cleaned']\n",
    "    else:\n",
    "        description = dic_taxonomy[label]['description_generated']\n",
    "                                        \n",
    "    prompt = f\"\"\"given document:\n",
    "'title'= '{row['title']}',\n",
    "'abstract'= '{row['abstract']}'\n",
    "'keywords'= '{row['keywords']}'\n",
    "\n",
    "Label:\n",
    "ID= {label_ID}\n",
    "name= '{cleaned_name}'\n",
    "description= '{description}'\n",
    "\"\"\"\n",
    "    return(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb6c867-2c03-4508-aa7e-78657f5842fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_format_label = [  \n",
    "    {  \n",
    "        \"name\": \"check_relevancy\",  \n",
    "        \"description\": \"check relevancy\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"main_focus\": {  \n",
    "                    \"type\": \"string\",  \n",
    "                    \"description\": \"main focus of the paper\",\n",
    "                },\n",
    "                \"label_fit\": {  \n",
    "                    \"type\": \"boolean\",  \n",
    "                    \"description\": \"if label is a good match for the paper\",\n",
    "                },\n",
    "            }  ,\n",
    "            \"required\" : [\"main_focus\" ,\"label_fit\"  ]\n",
    "\n",
    "        }  \n",
    "    }  \n",
    "]  \n",
    "\n",
    "def F_prompt_GPT_label(row):\n",
    "    message_user = F_prompt_label(row)\n",
    "    message_system = prompt_system_template_label\n",
    "\n",
    "    return(generate_openai_response(message_user, message_system, json_format_label, max_tokens=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb29603d-ceda-4d46-846c-2494aa50ff55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "executor_label = ThreadPoolExecutor(max_workers=2) \n",
    "def apply_async_label(df):  \n",
    "    GPT_answers = {}\n",
    "\n",
    "    futures_to_index = {executor_label.submit(F_prompt_GPT_label, row): index for index, row in df.iterrows()}  \n",
    "    for future in as_completed(futures_to_index):  \n",
    "        index = futures_to_index[future]  \n",
    "        print(index,end= ',')\n",
    "        GPT_answers[index] = future.result() \n",
    "    print(' end of GPT calls A.')\n",
    "    return(GPT_answers)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5c7ad0-089a-43d0-9910-f3e033494871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2- Ask about the label's Parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81dd5c1-2473-47a5-ad05-4528e5ab0c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_system_template_parent_without = \"You are an AI, trained to assess the potential relevance of a label for a given SSRN pre-print document. You'll be provided with the document's title, keywords, abstract, and the label's name. Your mission is to gauge if the label could be a reasonable match for the document. A label can be considered a reasonable match even if it only partially aligns with the document's main theme. Your response should be a JSON object. This JSON object should include three keys: \\\"main_focus\\\", a brief summary of the document's main theme, \\\"label_fit\\\", indicating the fit as a boolean value, and \\\"relevancy_score\\\", showing the relevance as a score from 0 to 1. It's important to use the full scoring range to indicate varying levels of relevance. Do not use the slash or backslash characters in your output.\"\n",
    "\n",
    "prompt_system_template_parent_with = \"You are an AI, trained to assess the potential relevance of a label for a given SSRN pre-print document. You'll be provided with the document's title, keywords, abstract, and the label's name and description. Your mission is to gauge if the label could be a reasonable match for the document. A label can be considered a reasonable match even if it only partially aligns with the document's main theme. Your response should be a JSON object. This JSON object should include three keys: \\\"main_focus\\\", a brief summary of the document's main theme, \\\"label_fit\\\", indicating the fit as a boolean value, and \\\"relevancy_score\\\", showing the relevance as a score from 0 to 1. It's important to use the full scoring range to indicate varying levels of relevance. Do not use the slash or backslash characters in your output.\"\n",
    "\n",
    "def F_prompt_parent (row) :\n",
    "    label = row['label']\n",
    "    label_ID = dic_taxonomy[label]['id']\n",
    "    cleaned_name = dic_taxonomy[label]['cleaned_name']\n",
    "    if len(dic_taxonomy[label]['description_cleaned'])>0:\n",
    "        description = dic_taxonomy[label]['description_cleaned']\n",
    "    else:\n",
    "        description = dic_taxonomy[label]['description_generated']\n",
    "                                        \n",
    "    prompt = f\"\"\"given document:\n",
    "'title'= '{row['title']}',\n",
    "'abstract'= '{row['abstract']}'\n",
    "'keywords'= '{row['keywords']}'\n",
    "\n",
    "Label:\n",
    "ID= {label_ID}\n",
    "name= '{cleaned_name}'\n",
    "\"\"\"\n",
    "    if len(description)>0:\n",
    "        prompt += f\"description= '{description}'\"\n",
    "        \n",
    "    return(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8e373d-6809-4d9b-bb3e-221255c18465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_format_parent = [  \n",
    "    {  \n",
    "        \"name\": \"check_relevancy\",  \n",
    "        \"description\": \"check relevancy\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"main_focus\": {  \n",
    "                    \"type\": \"string\",  \n",
    "                    \"description\": \"main focus of the paper\",\n",
    "                },\n",
    "                \"label_fit\": {  \n",
    "                    \"type\": \"boolean\",  \n",
    "                    \"description\": \"if label is a good match for the paper\",\n",
    "                },\n",
    "            }  ,\n",
    "            \"required\" : [\"main_focus\" ,\"label_fit\"  ]\n",
    "\n",
    "        }  \n",
    "    }  \n",
    "]  \n",
    "\n",
    "def F_prompt_GPT_parent(row):\n",
    "    message_user = F_prompt_parent(row)\n",
    "\n",
    "    if \"description= \" in message_user:\n",
    "        message_system = prompt_system_template_parent_with\n",
    "    else:\n",
    "        message_system = prompt_system_template_parent_without\n",
    "\n",
    "    return(generate_openai_response( message_user, message_system, json_format_parent, max_tokens=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d676d8d-8487-4dc4-8e84-08f63c22bef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "executor_parent = ThreadPoolExecutor(max_workers=4) \n",
    "def apply_async_parent(df):  \n",
    "    GPT_answers = {}\n",
    "\n",
    "    futures_to_index = {executor_parent.submit(F_prompt_GPT_parent, row): index for index, row in df.iterrows()}  \n",
    "    for future in as_completed(futures_to_index):  \n",
    "        index = futures_to_index[future]  \n",
    "        print(index,end= ',')\n",
    "        GPT_answers[index] = future.result() \n",
    "    print(' end of GPT calls B.')\n",
    "    return(GPT_answers)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc76452e-9fd1-4319-8f3f-8eef4273a491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ASK GPT In parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d7085d-bfad-4bb0-b59f-0d36b910de8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_in['labels_Proposed_approach'] = [[] for _ in range(len(df_in))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dac89c6-cf32-4c92-86b0-2a43fc9c733c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A"
    }
   },
   "outputs": [],
   "source": [
    "n_initial = 40\n",
    "\n",
    "for i_doc,doc_tmp in df_in.iloc[:].iterrows():\n",
    "    print('document' , i_doc, end = ': ')\n",
    "    # getting the inital labels\n",
    "    \n",
    "    #1 - Label itself\n",
    "    # Make a temporary dataframe with all labels\n",
    "    leave_labels = doc_tmp['labels_BiEncoder'][:n_initial]\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['label'] = leave_labels\n",
    "    df_temp['title']= doc_tmp['title']\n",
    "    df_temp['keywords'] = doc_tmp['keywords']\n",
    "    df_temp['abstract']\t= doc_tmp['abstract']\n",
    "    # call GPT in parallel\n",
    "    GPT_answer_labels = apply_async_label(df_temp)\n",
    "    selected_labels = [leave_labels[k] for k,v in GPT_answer_labels.items() if v['label_fit']]\n",
    "    print('-->' , len(selected_labels), end = ' left : ')\n",
    "    #2 - Parent Label\n",
    "    time.sleep(10)\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['label'] = [dic_taxonomy[l]['parent_label'] for l in selected_labels]\n",
    "    df_temp['title']= doc_tmp['title']\n",
    "    df_temp['keywords'] = doc_tmp['keywords']\n",
    "    df_temp['abstract']\t= doc_tmp['abstract']\n",
    "    \n",
    "    GPT_answer_parents = apply_async_parent(df_temp)\n",
    "    selected_labels_selected_parent = [selected_labels[k] for k,v in GPT_answer_parents.items() if v['label_fit']]\n",
    "    print('-->' , len(selected_labels_selected_parent))\n",
    "\n",
    "    #\n",
    "    df_in.at[i_doc,'labels_Proposed_approach'] = selected_labels_selected_parent\n",
    "    print(' . . . ')\n",
    "\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b035a997-5968-4d83-b1f0-8419300a3bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2321c8d-afb6-49fa-96dc-fe2d3f131a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_in.to_pickle(path_results + '5-PrpposedApproach.pckl')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05- ProposeApproach",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
